---
title: Troubleshooting MySQL for Pivotal Platform
owner: MySQL
---

<strong><%= modified_date %></strong>

This topic provides operators with basic instructions for troubleshooting on-demand MySQL for Pivotal Cloud Foundry (PCF).
For information about temporary MySQL for PCF service interruptions, see
<a href="./upgrade.html#interruptions">Service Interruptions</a>.

## <a id="errors"></a> Troubleshoot Errors

This section provides information on how to troubleshoot specific errors or error messages.

### <a id="x-product"></a> Common Services Errors

The following errors occur in multiple services:

+ [Failed Installation](#install-fail)
+ [Cannot Create or Delete Service Instances](#cannot-create-delete)
+ [Broker Request Timeouts](#timeouts)
+ [Instance Does Not Exist](#instance-not-exist)
+ [Cannot Bind to or Unbind from Service Instances](#cannot-bind)
+ [Cannot Connect to a Service Instance](#cannot-connect)
+ [Upgrade All Service Instances Errand Fails](#upgrade-all-fails)
+ [Missing Logs and Metrics](#missing-logs)

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-err-install-fail' %>

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-err-cannot-create-delete' %>

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-err-timeouts' %>

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-err-instance-not-exist' %>

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-err-other-errors' %>

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-err-cannot-connect' %>

<p class="note"><strong>Note:</strong>
  Service instances can also become temporarily inaccessible during upgrades and VM or network failures.
  See <a href="./upgrade.html#interruptions">Service Interruptions</a> for more information.
</p>

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-err-upgrade-all-fails' %>

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-err-missing-logs' %>


### <a id="l-f-errors"></a> Leader-Follower Service Instance Errors

This section provides solutions for the following errands:

+ [Unable to Determine Leader and Follower](#unable-to-determine)
+ [Both Leader and Follower Instances Are Writable](#both-writable)
+ [Both Leader and Follower Instances Are Read-Only](#both-read-only)

<%= partial vars.path_to_partials + "/troubleshoot-template", locals: {
    id: "unable-to-determine",
    description: "Unable to Determine Leader and Follower",
    symptom: <<~DESC,
    This problem happens when the <code>configure-leader-follower</code>
    errand fails because it cannot determine the VM roles. <br><br>

    The <code>configure-leader-follower</code> errand exits with <code>1</code>
    and the errand logs contain the following:

    <pre class="terminal">$ Unable to determine leader and follower based on transaction history.</pre>
    DESC

    cause: <<~DESC,

      Something has happened to the instances, such as a failure or manual intervention.
      As a result, there is not enough information available to determine the correct state
      and topology without operator intervention to resolve the issue.
      DESC

    solution: 'Use the <code>inspect</code> errand to determine which instance should be the leader. Then, using the
    <a href="./about-leader-follower.html#errands">orchestration</a>
    errands and backup/restore, you can put the service instance into a safe topology, and then rerun the
    <code>configure-leader-follower</code> errand. This is shown in the example below.<br><br>
    This example shows one outcome that the <code>inspect</code> errand can return: <br>

    <ol>
      <li>Use the
        <code>inspect</code>
        errand to retrieve relevant information about the two VMs:
        <pre class="terminal">$ bosh -e my-env -d my-dep run-errand inspect
      [...]
      Instance   mysql/4ecad54b-0704-47eb-8eef-eb228cab9724
      Exit Code  0
      Stdout     -
      Stderr   2017/12/11 18:25:54 Started executing command: inspect
               2017/12/11 18:25:54 Started GET https<span>:</span>//127.0.0.1:8443/status
               2017/12/11 18:25:54
               Has Data: false
               Read Only: true
               GTID Executed: 1d774323-de9e-11e7-be01-42010a001014:1-25
               Replication Configured: false
      Instance   mysql/e0b94ade-0114-4d49-a929-ce1616d8beda
      Exit Code  0
      Stdout     -
      Stderr   2017/12/11 18:25:54 Started executing command: inspect
               2017/12/11 18:25:54
               Started GET https<span>:</span>//127.0.0.1:8443/status
               2017/12/11 18:25:54
               Has Data: true
               Read Only: true
               GTID Executed: 1d774323-de9e-11e7-be01-42010a001014:1-25
               Replication Configured: true
      2 errand(s)
      Succeeded</pre>
        In the above scenario, the first instance is missing data but does not have replication configured. The second instance has data, and also has replication configured. The instructions below resolve this by copying data to the first instance, and
        resuming replication.</li>
      <li>Take a backup of the second instance using the
        <a href="./backup-mysqldump.html#create-backup">Create a ' + vars.product_old + ' Logical Backup</a>
        steps.</li>
      <li>Restore the backup artifact to the first instance using the
        <a href="./backup-mysqldump.html#restore-backup">Restore from a ' + vars.product_old + ' Logical Backup</a>
        steps.<br>
        At this point, the instances have equivalent data.</li>
      <li>
        Run the <code>configure-leader-follower</code> errand to reconfigure replication:
        <pre>bosh -e ENVIRONMENT -d DEPLOYMENT &#92;
          run-errand configure-leader-follower &#92;
          --instance=mysql/GUID-OF-LEADER</pre>
        For example:

        <pre class="terminal">$ bosh -e my-env -d my-dep &#92;
          run-errand configure-leader-follower &#92;
          --instance=mysql/4ecad54b-0704-47eb-8eef-eb228cab9724</pre>
      </li>
    </ol>'

} %>

<%= partial vars.path_to_partials + "/troubleshoot-template", locals: {
    id: "both-writable",

    description: "Both Leader and Follower Instances Are Writable",

    symptom: <<~DESC ,
    This problem happens when the<code>configure-leader-follower</code> errand fails because both VMs are writable and the VMs might hold differing data. <br><br>
    The <code>configureâ€“leader-follower</code> errand exits with <code>1</code>
    and the errand logs contain the following:

    <pre class="terminal">$ Both mysql instances are writable. Please ensure no divergent data and set one instance to read-only mode.</pre>
    DESC

    cause:
      vars.product_old + ' tries to ensure that there is only one writable instance of the
      leader-follower pair at any given time. However, in certain situations, such as
      network partitions, or manual intervention outside of the provided bosh
      errands, it is possible for both instances to be writable. <br><br>
      The service instances remain in this state until an operator resolves the issue
      to ensure that the correct instance is promoted and reduce the potential for data divergence.
    ',

    solution: '
      <ol>
        <li>
          Use the
          <code>inspect</code> errand to retrieve the GTID Executed set for each VM:

          <pre class="terminal">$ bosh -e my-env -d my-dep run-errand inspect
        [...]
        Instance   mysql/4ecad54b-0704-47eb-8eef-eb228cab9724
        Exit Code  0
        Stdout     -
        Stderr     2017/12/11 18:25:54 Started executing command: inspect
                 2017/12/11 18:25:54 Started GET https<span>:</span>127.0.0.1:8443/status
                 2017/12/11 18:25:54
                 Has Data: true
                 Read Only: false
                 GTID Executed: 1d774323-de9e-11e7-be01-42010a001014:1-23
                 Replication Configured: false<br /><br />
        Instance   mysql/e0b94ade-0114-4d49-a929-ce1616d8beda
        Exit Code  0
        Stdout     -
        Stderr     2017/12/11 18:25:54 Started executing command: inspect
                 2017/12/11 18:25:54 Started GET https<span>:</span>127.0.0.1:8443/status
                 2017/12/11 18:25:54
                 Has Data: true
                 Read Only: false
                 GTID Executed: 1d774323-de9e-11e7-be01-42010a001014:1-25
                 Replication Configured: false<br /><br />
        2 errand(s)<br />
        Succeeded
      </pre>
          If the GTID Executed sets for both instances are the same, continue to Step 2. If they are different, continue to Step 4.</li>
        <li>Look at the value of GTID Executed for both instances.
          <ul>
            <li>If the range after the GUID is equivalent, either instance can be made read-only, as described in Step 3.</li>
            <li>If one instance has a range that is a subset of the other, the instance with the subset should must be made read-only, as described in Step 3.</li>
          </ul>
        </li>
        <li>Based on the information you gathered in the step above, run the
          <code>make-read-only</code>
          errand to make the appropriate instance read-only:
          <pre><code>bosh -e ENVIRONMENT -d DEPLOYMENT &#92;
            run-errand make-read-only &#92;
            --instance=mysql/MYSQL-SUBSET-INSTANCE</code></pre>
          For example:
          <pre class="terminal">$ bosh -e my-env -d my-dep &#92;
              run-errand make-read-only &#92;
              --instance=mysql/e0b94ade-0114-4d49-a929-ce1616d8beda
            [...]
            succeeded</pre>
        </li>
        <li>If the GTID Executed sets are neither equivalent nor subsets, data has diverged and you must determine what data has diverged as part of the procedure below:
          <ol>
            <li>Use the
              <code>make-read-only</code>
              errand to set both instances to read-only to prevent further data divergence.
              <pre><code>bosh -e ENVIRONMENT -d DEPLOYMENT &#92;
                run-errand make-read-only &#92;
                --instance=mysql/MYSQL-INSTANCE</code></pre>
              For example:
              <pre class="terminal">$ bosh -e my-env -d my-dep &#92;
                  run-errand make-read-only &#92;
                  --instance=mysql/e0b94ade-0114-4d49-a929-ce1616d8beda
                  [...]
                  succeeded</pre>
            </li>
            <li>Take a backup of both instances using the
              <a href="./backup-mysqldump.html#create-backup"> Create a ' + vars.product_old + ' Logical Backup</a>
              steps.</li>
            <li>Manually inspect the data on each instance to determine the discrepancies and put the data on the instance that is further ahead---this instance has the higher GTID Executed set, and will be the new leader.</li>
            <li>Migrate all appropriate data to the new leader instance.</li>
            <li>After putting all data on the leader, ssh onto the follower:
              <pre><code>bosh -e ENVIRONMENT -d DEPLOYMENT ssh mysql/GUID-OF-FOLLOWER</code></pre>
              For example:
              <pre class="terminal">$ bosh -e my-env -d my-dep ssh mysql/e0b94ade-0114-4d49-a929-ce1616d8beda</pre>
            </li>
            <li>Become root with the command
              <code>sudo su</code>.<br></li>
            <li>Stop the mysql process with the command
              <code>monit stop mysql</code>.</li>
            <li>Delete the data directory of the follower with the command
              <code>rm -rf /var/vcap/store/mysql</code>.</li>
            <li>Start the mysql process with the command
              <code>monit start mysql</code>.</li>
            <li>Use the
              <code>configure-leader-follower</code>
              errand to copy the leader data to the follower and resume replication:
              <pre><code>bosh -e ENVIRONMENT -d DEPLOYMENT &#92;
                run-errand configure-leader-follower &#92;
                --instance=mysql/GUID-OF-LEADER</code></pre>
              For example:
              <pre class="terminal">$ bosh -e my-env -d my-dep &#92;
                run-errand configure-leader-follower &#92;
                --instance=mysql/4ecad54b-0704-47eb-8eef-eb228cab9724</pre>
            </li>
          </ol>
        </li>
      </ol>
    '
} %>

<%= partial vars.path_to_partials + "/troubleshoot-template", locals: {
    id: "both-read-only",
    description: "Both Leader and Follower Instances Are Read-Only",
    symptom: "Developers report that apps cannot write to the database. In a leader-follower topology, the leader VM is writable and the follower VM is read-only.
    However if both VMs are read only, apps cannot write to the database.",
    cause: "This problem happens if the leader VM fails and the BOSH Resurrector is enabled.
    When the leader is resurrected, it is set as read-only.",
    solution:'
    <ol>
      <li>
        Use the
        <code>inspect</code>
        errand to confirm that both VMs are in a read-only state:
        <pre>bosh -e ENVIRONMENT -d DEPLOYMENT run-errand inspect</pre>
      </li>
      <li>Examine the output and locate the information about the leader-follower ' +
        vars.product_old+ ' VMs:
        <pre class="terminal">
    Instance   mysql/4eexample54b-0704-47eb-8eef-eb2example724
    Exit Code  0
    Stdout     -
    Stderr     2017/12/11 18:25:54 Started executing command: inspect
             2017/12/11 18:25:54 Started GET https<span>:</span>999.0.0.1:8443/status
             2017/12/11 18:25:54
             Has Data: true
             Read Only: true
             GTID Executed: 1d779999-de9e-11e7-be01-42010a009999:1-23
             Replication Configured: true<br /><br />
    Instance   mysql/e0exampleade-0114-4d49-a929-cexample8beda
    Exit Code  0
    Stdout     -
    Stderr     2017/12/11 18:25:54 Started executing command: inspect
             2017/12/11 18:25:54 Started GET https<span>:</span>999.0.0.1:8443/status
             2017/12/11 18:25:54
             Has Data: true
             Read Only: true
             GTID Executed: 1d779999-de9e-11e7-be01-42010a009999:1-25
             Replication Configured: false<br /><br />
    2 errand(s)<br />
    Succeeded
    </pre>
      </li>
      <li>
        If Read Only is set to
        <code>true</code>
        for both VMs, make the leader writable using the following command:
        <pre>bosh -e ENVIRONMENT -d DEPLOYMENT &#92;
          run-errand configure-leader-follower &#92;
          --instance=mysql/GUID-OF-LEADER</pre>
        For example, if the second instance above is the leader:
        <pre class="terminal">
    $ bosh -e my-env -d my-dep &#92;
      run-errand configure-leader-follower &#92;
      --instance=mysql/e0exampleade-0114-4d49-a929-cexample8beda
    </pre>
      </li>
    </ol>
   '
} %>

###<a id="apps-inoperable"></a>Inoperable App Errors

This section provides a solution for the following errand:

+ [Persistent Disk is Full](#persistent-disk)

<%= partial vars.path_to_partials + "/troubleshoot-template", locals: {
    id: "persistent-disk",
    description: "Persistent Disk is Full",
    symptom: 'Developers report that read, write, and cf CLI operations do not work.
    Developers cannot upgrade to a larger ' + vars.product_old + ' service plan to free up disk space. <br><br>
    If your persistent disk is full, apps become inoperable.
    In this state, read, write, and Cloud Foundry Command-Line Interface (cf CLI) operations do not work.',
    cause: 'This problem happens if your persistent disk is full.
    When you use the BOSH CLI to target your deployment, you see that instances are at 100% persistent disk usage. <br><br>
    Available disk space can be increased by deleting log files.
    After deleting logs, you can then upgrade to a larger ' + vars.product_old + ' service plan.',
    solution: '
      To resolve this issue, you must free up disk space by deleting binary logs.<br><br>

      For instructions for deleting binary logs, see
      <a href="https://community.pivotal.io/s/article/mysql-for-pcf-hangs-when-server-vm-persistent-disk-is-full">MySQL for PCF hangs when server VM persistent disk is full</a>
      in the Pivotal Support knowledge base.
      <p class="note warning"><strong>Warning:</strong>
        Deleting binary logs is a destructive procedure and can result in MySQL data loss.
        Only do this procedure with the assistance of <a href="https://pivotal.io/support">Pivotal Support</a>.
      </p>'
} %>


###<a id="ha"></a>Highly Available Cluster Errors

This section provides solutions for the following errands:

+ [Unresponsive Node in a Highly Available Cluster](#unresponsive)
+ [Many Replication Errors in Logs for Highly Available Clusters](#replication-errors)

<%= partial vars.path_to_partials + "/troubleshoot-template", locals: {
    id: "unresponsive",
    description: "Unresponsive Node in a Highly Available Cluster",
    symptom:'
    A client connected to a ' + vars.product_old + ' cluster node reports the following error:

    <pre class="terminal">WSREP has not yet prepared this node for application use</pre>
    Some clients might instead return the following:
    <pre class="terminal">
      unknown error
    </pre>
    ',

    cause: 'If the client is connected to a ' + vars.product_old + ' cluster node and that node loses connection to the rest of the cluster, the node stops accepting writes. If the connection to this node is made through the proxy, the proxy automatically
    re-routes further connections to a different node.',

    solution: <<~DESC,
    A node can become unresponsive for a number of reasons. For solutions, see the following:

    <ul>
      <li>
        <strong>Network Latency</strong>: If network latency causes a node to become unresponsive, the node drops but eventually rejoins. The node automatically rejoins only if one node has left the cluster. Consult your IaaS network settings to reduce your network latency. <hr></li>
      <li>
        <strong>MySQL Process Failure:</strong> If the MySQL process crashes, <code>monit</code>
        and BOSH should restore the process. If the process is not restored, run the
        <code>download-logs</code> tool and consult the error logs it generates. For more information, see the
        <a href="#download-logs">download-logs</a> section below.<hr>
      </li>
      <li>
        <strong>Firewall Rule Change:</strong>
        If your firewall rules change, it might prevent a node from communicating with the rest of the cluster. This causes the node to become unresponsive. In this case, the logs show the node leaving the cluster but do not show network latency errors.
        <br><br>
        To confirm that the node is unresponsive because of a firewall rule change, SSH from a responsive node to the unresponsive node. If you cannot connect, the node is unresponsive due to a firewall rule change. Change your firewall rules to enable the
        unresponsive node to rejoin the cluster.<hr>
      </li>
      <li>
        <strong>VM Failure:</strong>
        If you cannot SSH into a node and you are not detecting either network latency or firewall issues, your node might be down due to VM failure. To confirm that the node is unresponsive and re-create the VM, see
        <a href="#recreate-VM">
          Re-create a Corrupted VM in a Highly Available</a>
        below.<hr>
      </li>
      <li>
        <strong>Node Unable to Rejoin:</strong> If a detached existing node fails to join the cluster, its
        <code>sequence_number</code> might be higher than those of the nodes with quorum. A higher
        <code>sequence_number</code> on the detached node indicates that it has recent changes to the
         data that the primary component lacks. You can check this by looking at the node's error log at
        <code>/var/vcap/sys/log/pxc-mysql/mysql.err.log</code>.
        <br><br>
        To restore the cluster, do one the following:
        <ul>
          <li>If the detached node has a higher sequence number than the primary component, do the procedures in
            <a href="./bootstrapping.html">Bootstrapping</a>.
          </li>
          <li>
            If bootstrapping does not restore the cluster, you can manually force
            the node to rejoin the cluster. This removes all of the unsynchronized data
            from the detached server node and creates a new copy of the cluster data on the node. For more
            information, see <a href="#manual-force">Force a Node to Rejoin a Highly Available Cluster Manually</a> below.
            <p class="note warning">
              <strong>Warning</strong>: Forcing a node to rejoin the cluster is a destructive procedure. Only do this procedure with the assistance of
              <a href="https://support.pivotal.io">Pivotal Support</a>.
            </p>
          </li>
        </ul>
      </li>
    </ul>
    DESC
} %>

<%= partial vars.path_to_partials + "/troubleshoot-template", locals: {
    id: "replication-errors",
    description: "Many Replication Errors in Logs for Highly Available Clusters",
    symptom: <<~DESC,
    You see many replication errors in the MySQL logs, like the following:

    <pre class="terminal">
    160318 9:25:16 [Warning] WSREP: RBR event 1 Query apply warning: 1, 16992456
    160318 9:25:16 [Warning] WSREP: Ignoring error for TO isolated action: source: abcd1234-abcd-1234-abcd-1234abcd1234 version: 3 local: 0 state: APPLYING flags: 65 conn_id: 246804 trx_id: -1 seqnos (l: 865022, g: 16992456, s: 16992455, d: 16992455, ts: 2530660989030983)
    160318 9:25:16 [ERROR] Slave SQL: Error 'Duplicate column name 'number'' on query. Default database: 'cf_0123456_1234_abcd_1234_abcd1234abcd'. Query: 'ALTER TABLE ...'
    </pre>
    DESC
    cause: "This problem happens when there are errors in SQL statements.",
    solution: <<~DESC,
      For solutions for replication errors in MySQL log files, see the table below.

      <table class="nice">
        <tr>
          <th>Additional Error</th>
          <th>Solution</th>
        </tr>
        <tr>
          <td>
            <code>ALTER TABLE</code>
            errors</td>
          <td>Fix the
            <code>ALTER TABLE</code>
            error.<br>
            This error can occur when an app issues an invalid data definition statement. Other nodes log this problem as a replication error because they fail to replicate the
            <code>ALTER TABLE</code>.</td>
        </tr>
        <tr>
          <td>Increased persistent disk usage or running out of working memory</td>
          <td>Decode the
            <code>GRA_*.log</code>
            files and look for errors. See
            <a href="https://community.pivotal.io/s/article/How-to-decode-Galera-GRA-logs-for-MySQL-for-pcf-v1-10">
              How to decode Galera GRA log files</a>
            in the Pivotal Support knowledge base. The GRA log files contain failing DDL statements.</td>
        </tr>
      </table>

      If you see replication errors, but no <code>ALTER TABLE</code>
      or persistent disk or memory issues, you can ignore the replication errors.
    DESC

} %>


##  <a id="components"></a>  Troubleshoot Components

This section provides guidance on checking for and fixing issues in on-demand service components.

### <a id="bosh"></a> BOSH Problems

#### <a id="large-queue"></a> Large BOSH Queue


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-comp-large-queue' %>


### <a id="bosh-config"></a> Configuration

#### <a id="bosh-instance-fail"></a> Service Instances in Failing State

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-comp-bosh-instance-fail' %>


###  <a id="auth"></a>  Authentication

#### <a id="uaa-change"></a> UAA Changes

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-comp-uaa-change' %>


###  <a id="network"></a>  Networking


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-comp-network' %>


####  <a id="broker-to-instances"></a>  Validate Service Broker Connectivity to Service Instances


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-comp-broker-to-instances' %>


####  <a id="app-to-instances"></a>  Validate App Access to Service Instance


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-comp-app-to-instances' %>


###  <a id="quotas"></a>  Quotas

#### <a id="plan-quotas"></a> Plan Quota Issues

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-comp-plan-quotas' %>


####  <a id="global-quotas"></a>  Global Quota Issues

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-comp-global-quotas' %>


###  <a id="failing-jobs"></a>  Failing Jobs and Unhealthy Instances

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-comp-failing-jobs' %>

A failing process or failing VM might come back automatically after a temporary service outage. See
<a href="./interruptions.html#process-fail">VM Process Failure</a>
and <a href="./interruptions.html#vm-fail">VM Failure</a>.

###  <a id="az-region-fail"></a>  AZ or Region Failure

Failures at the IaaS level, such as Availability Zone (AZ) or region failures,
can interrupt service and require manual restoration.
See <a href="./interruptions.html#az-fail">AZ Failure</a> and <a href="./interruptions.html#region-fail">Region Failure</a>.

##  <a id="techniques"></a>  Techniques for Troubleshooting

Instructions on interacting with the on-demand service broker and on-demand service
instance BOSH deployments, and on performing general maintenance and housekeeping tasks

### <a id="parse-error"></a> Parse a Cloud Foundry (CF) Error Message

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-parse-error' %>


###  <a id="bosh-cf-access"></a>  Access Broker and Instance Logs and VMs


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-bosh-cf-access' %>


####  <a id="access-broker"></a>  Access Broker Logs and VMs


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-access-broker' %>


####  <a id="access-instance"></a>  Access Service Instance Logs and VMs


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-access-instance' %>

### <a id="broker-errands"></a> Run Service Broker Errands to Manage Brokers and Instances

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-broker-errands' %>

#### <a id="register-broker"></a> Register Broker

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-register-broker' %>

#### <a id="deregister-broker"></a> Deregister Broker

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-deregister-broker' %>

#### <a id="upgrade-all"></a> Upgrade All Service Instances

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-upgrade-all' %>

#### <a id="delete-all"></a> Delete All Service Instances

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-delete-all' %>

### <a id="detect-orphans"></a> Detect Orphaned Service Instances

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-detect-orphans' %>

### <a id="instance-creds"></a> Retrieve Admin and Read-Only Admin Credentials for a Service Instance

<%= partial vars.path_to_partials + "/services-tshoot/tshoot-tech-instance-creds" %>

###  <a id="reinstall"></a>  Reinstall a Tile

To reinstall the MySQL for PCF tile, see <a href="https://community.pivotal.io/s/article/Reinstalling-MySQL-for-Pivotal-Cloud-Foundry-version-2-and-above">Reinstalling MySQL for Pivotal Cloud Foundry version 2 and above</a>
in the Pivotal Support knowledge base.

###  <a id="view-resources"></a>  View Resource Saturation and Scaling


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-view-resources' %>


###  <a id="id-instance-owner"></a> Identify Apps using a Service Instance


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-id-instance-owner' %>


###  <a id="monitor-quota"></a>  Monitor Quota Saturation and Service Instance Count


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-tech-monitor-quota' %>


###  <a id="trouble-ha"></a>Techniques for Troubleshooting Highly Available Clusters

If your cluster is experiencing downtime or in a degraded state, Pivotal recommends
gathering information to diagnose the type of failure the cluster is experiencing with  the following workflow:

<ol>
  <li>Consult solutions for common errors. See
    <a href="#ha">Highly Available Cluster Troubleshooting Errors</a>
    above.
  </li>
  <li>Use
    <code>mysql-diag</code>
    to view a summary of the network, disk, and replication state of each cluster node. Depending on the output from
    <code>mysql-diag</code>, you might recover your cluster with the following troubleshooting techniques:
    <ul>
      <li>To force a node to rejoin the cluster, see
        <a href="#manual-force">Force a Node to Rejoin a Highly Available Cluster Manually</a>
        below.</li>
      <li>To re-create a corrupted VM, see
        <a href="#recreate-VM">Re-create a Corrupted VM in a Highly Available Cluster</a>
        below.</li>
      <li>To check if replication is working, see
        <a href="#check-replication">Check Replication in a Highly Available Cluster</a>
        below.</li>
    </ul>
    For more information about
    <code>mysql-diag</code>, see
    <a href="./mysql-diag.html">Running mysql-diag</a>.
  </li>

  <li>
    Run <code>download-logs</code> against each node in your MySQL for PCF cluster, proxies, and jumpbox VM.
    You must run
    <code>download-logs</code> before attempting recovery because any failures in the recovery procedure can result in logs being lost or made inaccessible.
    <br>
    For more information, see the <a href="#download-logs">download-logs</a>
    section below.
    <br>
    <p class="note">
      <strong>Note:</strong>
      Pivotal recommends that you use the <code>-X</code> flag to get the complete
      set of available logs. However, if your cluster processes a high volume of
      transactions, the complete set might be too large and you can omit this flag
      to fetch the essential set of logs.
    </p>
  </li>

  <li>
    If you are uncertain about the recovery steps to take, submit a ticket through
    <a href="https://support.pivotal.io/">Pivotal Support</a>. When you submit a ticket provide the following information:
    <ul>
      <li>
        <strong>mysql-diag output:</strong>
        A summary of the network, disk, and replication state. The
        <a href="./mysql-diag.html">Running mysql-diag</a>
        topic explains how to run
        <strong>mysql-diag</strong>.
      </li>
      <li>
        <strong>download-logs logs:</strong>
        Logs from your MySQL for PCF cluster, proxies, and jumpbox VM. The
        <a href="#download-logs" ">download-logs</a>
        section below explains how to run
        <strong>download-logs</strong>.
      </li>
      <li>
        <strong>Deployment environment:</strong>
        The environment that MySQL for PCF is running in such as <%= vars.app_runtime_full %> or a service tile.</li>
      <li>
        <strong>Version numbers:</strong>
        The versions of the installed <%= vars.ops_manager %>, PAS, and MySQL for PCF
      </li>
    </ul>
  </li>
</ol>

<p class="note warning">
  <strong>Warning:</strong>
  Do not attempt to resolve cluster issues by reconfiguring the cluster, such as changing the number of nodes or networks. Only follow the diagnosis steps in this document. If you are unsure how to proceed, contact
  <a href="https://support.pivotal.io/">Pivotal Support</a>.
</p>


### <a id="manual-force"></a> Force a Node to Rejoin a Highly Available Cluster Manually

If a detached node fails to rejoin the cluster after a configured grace period,
you can manually force the node to rejoin the cluster.
This procedure removes all the data on the node, forces the node to join the cluster,
and creates a new copy of the cluster data on the node.


<p class="note warning">
  <strong>Warning:</strong>
  If you manually force a node to rejoin the cluster, data stored on the local node is lost. Do not force nodes to rejoin the cluster if you want to preserve unsynchronized data. Only do this procedure with the assistance of
  <a href="https://support.pivotal.io">Pivotal Support</a>.
</p>

Before following this procedure, try to bootstrap the cluster. For more information, see
<a href="./bootstrapping.html">Bootstrapping</a>.
<br><br>
To manually force a node to rejoin the cluster, do the following:

<ol>
  <li>
    SSH into the node by following the procedure in
    <a href="https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#bosh-ssh">BOSH SSH</a>.
  </li>
  <li>
    Become root by running:
    <pre> sudo su</pre>
  </li>
  <li>
    Shut down the
    <code>mysqld</code>
    process on the node by running:
    <pre>  monit stop galera-init </pre>
  </li>
  <li>
    Remove the unsynchronized data on the node by running:
    <pre>  rm -rf /var/vcap/store/pxc-mysql</pre>
  </li>
  <li>
    Prepare the node before restarting by running:
    <pre>  /var/vcap/jobs/pxc-mysql/bin/pre-start</pre>
  </li>
  <li>Restart the
    <code>mysqld</code>
    process by running:
    <pre> monit start galera-init</pre>
  </li>
</ol>


### <a id="recreate-VM"></a>Re-create a Corrupted VM in a Highly Available Cluster

To re-create a corrupted VM:

<ol>
  <li>To log in to the BOSH Director VM by doing the following procedures:
    <ol>
      <li>Gather the information needed to log in to the BOSH Director VM by doing the procedure in
        <a href="https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#gather">Gather Credential and IP Address Information</a>&#46;</li>
      <li>Log in to the <%= vars.ops_manager %> VM by doing the procedure in
        <a href="https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#ssh">Log in to the <%= vars.ops_manager %> VM with SSH</a>&#46;</li>
      <li>Log in to the BOSH Director VM by doing the procedure in
        <a href="https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#log-in">Log in to the BOSH Director VM</a>&#46;</li>
    </ol>
  </li>
  <li>
    Identify and re-create the unresponsive node with
    <code>bosh cloudcheck</code>, by doing the procedure in
    <a href="https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#cck">BOSH Cloudcheck</a>
    and run
    <code>Recreate VM using last known apply spec</code>.
    <p class="note warning">
      <strong>Warning:</strong>
      Recreating a node will clear its logs. Ensure the node is completely down before recreating it.
    </p>
    <p class="note warning">
      <strong>Warning:</strong>
      Only re-create one node. Do not re-create the entire cluster. If more than one node is down, contact
      <a href="https://support.pivotal.io">Pivotal Support</a>.
    </p>
  </li>
</ol>


### <a id="check-replication"></a>Check Replication Status in a Highly Available Cluster

If you see stale data in your cluster, you can check whether replication is functioning normally.


To check the replication status, do the following:

<ol>
  <li>To log in to the BOSH Director VM, do the following:
    <ol>
      <li>Gather the information needed to log in to the BOSH Director VM by doing the
        procedure in
        <a href="https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#gather">Gather Credential and IP Address Information</a>.
      </li>
      <li>Log in to the <%= vars.ops_manager %> VM by doing the procedure in
        <a href="https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#ssh">Log in to the <%= vars.ops_manager %> VM with SSH</a>.
      </li>
    </ol>
  </li>
  <li>
    Create a dummy database in the first node by running:
    <pre>mysql -h FIRST-NODE-IP-ADDRESS \
       -u YOUR-IDENTITY \
       -p -e "create database verify_healthy;"</pre>
    Where:
    <ul>
      <li>
        <code>FIRST-NODE-IP-ADDRESS</code>
        is the IP address of the first node you recorded in step 1.
      </li>
      <li>
        <code>YOUR-IDENTITY</code> is the value of <code>identity</code>
        that you recorded in step 1.
      </li>
    </ul>
  </li>
  <li>
    Create a dummy table in the dummy database by running:
    <pre>mysql -h FIRST-NODE-IP-ADDRESS \
      -u your-identity \
      -p -D verify_healthy \
      -e "create table dummy_table (id int not null primary key auto_increment, info text) \
      engine='innodb';"</pre>
  </li>
  <li>
    Insert data into the dummy table by running:
    <pre>mysql -h FIRST-NODE-IP-ADDRESS \
      -u YOUR-IDENTITY \
      -p -D verify_healthy \
      -e "insert into dummy_table(info) values ('dummy data'),('more dummy data'),('even more dummy data');"</pre>
  </li>
  <li>
    <p>
      Query the table and verify that the three rows of dummy data exist on the first node by running:
    </p>
    <pre>mysql -h FIRST-NODE-IP-ADDRESS \
      -u YOUR-IDENTITY \
      -p -D verify_healthy \
      -e "select * from dummy_table;"</pre>

    When prompted for a password, provide the <code>password</code>
    value recorded in step 1.<br>
    The above command returns output similar to the following:
    <pre class="terminal">
  +----+----------------------+
  | id | info                 |
  +----+----------------------+
  |  4 | dummy data           |
  |  7 | more dummy data      |
  | 10 | even more dummy data |
  +----+----------------------+</pre>
  </li>
  <li>
    Verify that the other nodes contain the same dummy data
    by doing the following for each of the remaining MySQL server IP addresses:
    <ol>
      <li>Query the dummy table by running :
        <pre>mysql -h NEXT-NODE-IP-ADDRESS \
          -u YOUR-IDENTITY \
          -p -D verify\_healthy \
          -e "select * from dummy_table;"</pre>

        When prompted for a password, provide the
        <code>password</code>
        value recorded in step 1.
      </li>
      <li>
        Verify that the node contains the same three rows of dummy data as the other nodes
        by running:

        <pre >mysql -h NEXT-NODE-IP-ADDRESS \
          -u YOUR-IDENTITY \
          -p -D verify\_healthy \
          -e "select * from dummy\_table;"</pre>

        When prompted for a password, provide the
        <code>password</code>
        value recorded in step
        </li>

        <li>Verify that the above command returns output similar to the following:
        <pre class="terminal">
    +----+----------------------+
    | id | info                 |
    +----+----------------------+
    |  4 | dummy data           |
    |  7 | more dummy data      |
    | 10 | even more dummy data |
    +----+----------------------+ </pre>
      </li>
    </ol>
  </li>
  <li>
    <strong>If each MySQL server instance does not return the same result,</strong> before proceeding
    further or making any changes to your deployment, contact
    <a href="https://support.pivotal.io/">Pivotal Support</a>
    <br>
    <strong>If each MySQL server instance returns the same result,</strong> then you can safely proceed
    to scaling down your cluster to a single node.
  </li>
</ol>


##  <a id="tools"></a>  Tools for Troubleshooting

The troubleshooting techniques described above use the following tools.

### <a id="download-logs"></a>download-logs

`download-logs` is a script that you can run from your <%= vars.ops_manager %> VM to aggregate
logs from your MySQL for PCF cluster nodes, proxies, and, with highly available clusters,
the jumpbox VM.

To use the `download-logs` script:

1. Download and unzip the `download-logs` script from [MySQL for PCF](https://network.pivotal.io/products/pivotal-mysql) on Pivotal Network.

1. From the <%= vars.ops_manager %> Installation Dashboard, navigate to **BOSH Director** > **Credentials**.

1. Click **Link to Credential** for the **Bosh Commandline Credentials**.

4. From the plaintext file that opens, record the values for the following:
  + `BOSH_CLIENT`
  + `BOSH_CLIENT_SECRET`
  + `BOSH_CA_CERT`
  + `BOSH_ENVIRONMENT`

1. From the BOSH CLI, view the name of the BOSH deployment for MySQL for PCF by running:

    ```
    bosh deployments
    ```
    Record the name of the BOSH deployment.

1. SSH into your <%= vars.ops_manager %> VM by doing the procedures in
[Gather Credential and IP Address Information](https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#gather)
and [SSH into <%= vars.ops_manager %>](https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#ssh).

1. File transfer or copy-paste the `download-logs` script to a working directory on the <%= vars.ops_manager %> VM.

1. Set local environment variables to the same BOSH variable values that you recorded earlier,
including `BOSH_DEPLOYMENT` for the deployment name.
  <br><br>
    For example:
    <pre class="terminal">$ BOSH\_CLIENT=ops\_manager \
      BOSH\_CLIENT\_SECRET=a123bc-E_4Ke3fb-gImbl3xw4a7meW0rY
      BOSH\_CA\_CERT=/var/tempest/workspaces/default/root\_ca\_certificate \
      BOSH\_ENVIRONMENT=10.0.0.5 \
      BOSH\_DEPLOYMENT=pivotal-mysql-14c4</pre>

1. Run the `download-logs` script by running:

    ```
    ./download-logs -o .
    ```

    The script saves a compressed file of logs combined from all MySQL for PCF VMs.
    The filename has the form `TIMESTAMP-mysql-logs.tar.gz.gpg`.

###  <a id="mysql-diag"></a> mysql-diag

The `mysql-diag` tools outputs the current status of a highly available (HA) MySQL for PCF cluster
in PCF and suggests recovery actions if the cluster fails.

For more information, see [Running mysql-diag](./mysql-diag.html).

## <a id="kb"></a> Knowledge Base (Community)

<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-kb' %>


##  <a id="support"></a>  File a Support Ticket


<!-- The below partial is in https://github.com/pivotal-cf/docs-partials -->

<%= partial '../../p-mysql/partials/services-tshoot/tshoot-support' %>
