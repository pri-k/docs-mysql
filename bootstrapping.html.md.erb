---
title: Bootstrapping
owner: MySQL
---

This topic describes how to bootstrap your MySQL cluster in the event of a cluster failure.

## <a id="when-to-bootstrap"></a> When to Bootstrap ##

To determine whether you need to bootstrap your cluster, you must check whether the cluster has lost quorum. Bootstrapping is only required when the cluster has lost quorum. See [Check Cluster State](#check-state) for more information about checking the state of your cluster.

Quorum is lost when less than half of the nodes can communicate with each other for longer than the configured grace period. In Galera terminology, if a node can communicate with the rest of the cluster, its database is in a good state, and it reports itself as ```synced```.

If quorum has not been lost, individual unhealthy nodes should automatically rejoin the cluster once repaired, which means the error is resolved, the node is restarted, or connectivity is restored.

To check whether your cluster has lost quorum, look for the following symptoms:

- All nodes appear "Unhealthy" on the proxy dashboard, as in the following screenshot:
![3 out of 3 nodes are unhealthy.](quorum-lost.png)
- All responsive nodes report the value of `wsrep_cluster_status` as `non-Primary` in the MySQL client.

        mysql> SHOW STATUS LIKE 'wsrep_cluster_status';
        +----------------------+-------------+
        | Variable_name        | Value       |
        +----------------------+-------------+
        | wsrep_cluster_status | non-Primary |
        +----------------------+-------------+
- All responsive nodes respond with `ERROR 1047` when using most statement types in the MySQL client:

        mysql> select * from mysql.user;
        ERROR 1047 (08S01) at line 1: WSREP has not yet prepared node for application use

## <a id="bootstrapping"></a>Prerequisites for Bootstrapping ##

Before running the bootstrapping procedures below, you must SSH into the Ops Manager VM and log in to the BOSH Director. For more information, see [Prepare to Use the BOSH CLI](https://docs.pivotal.io/pivotalcf/1-11/customizing/trouble-advanced.html#prepare).

<p class="note"><strong>Note:</strong> The examples in these instructions reflect a three-node MySQL for Pivotal Cloud Foundry (PCF) deployment. The process to bootstrap a two-node plus an arbitrator is identical, but the output will not match the examples.</p>

## <a id="assisted-bootstrap"></a>Assisted Bootstrap ##

MySQL for PCF v1.8.0 and later include a [BOSH errand](http://bosh.io/docs/jobs.html#jobs-vs-errands) to automate the process of bootstrapping. It is still necessary to manually initiate the bootstrap process, but using this errand reduces the number of manual steps necessary to complete the process.

In most cases, running the errand is sufficient, however there are some conditions which require additional steps.

#### <a id="how-it-works"></a>How It Works ####

The bootstrap errand simply automates the steps in the manual bootstrapping process documented below. It finds the node with the highest transaction sequence number, and asks it to start up by itself (i.e. in bootstrap mode), then asks the remaining nodes to join the cluster.

## <a id="cluster-disrupted"></a>Scenario 1: Virtual Machines Running, Cluster Disrupted ##

In this scenario, the nodes are up and running, but the cluster has been disrupted.

To determine whether the cluster has been disrupted, use the BOSH CLI to list the jobs and see if they are `failing`. Use one of these commands depending on your Ops Manager version:<br>

<ul>
    <li><strong>Ops Manager v1.11 or later:</strong> <code>bosh2 -e YOUR-ENV instances</code> </li>
    <li><strong>Ops Manager v1.10:</strong> <code>bosh vms</code></li>
</ul>


<a id="vms"></a>The output resembles the following:

<pre class="terminal">
Instance                                                             Process State  AZ             IPs
backup-prepare/c635410e-917d-46aa-b054-86d222b6d1c0                  running        us-central1-b  10.0.4.47
bootstrap/a31af4ff-e1df-4ff1-a781-abc3c6320ed4                       -              us-central1-b  -
broker-registrar/1a93e53d-af7c-4308-85d4-3b2b80d504e4                -              us-central1-b  10.0.4.58
cf-mysql-broker/137d52b8-a1b0-41f3-847f-c44f51f87728                 running        us-central1-c  10.0.4.57
cf-mysql-broker/28b463b1-cc12-42bf-b34b-82ca7c417c41                 running        us-central1-b  10.0.4.56
deregister-and-purge-instances/4cb93432-4d90-4f1d-8152-d0c238fa5aab  -              us-central1-b  -
monitoring/f7117dcb-1c22-495e-a99e-cf2add90dea9                      running        us-central1-b  10.0.4.48
mysql/220fe72a-9026-4e2e-9fe3-1f5c0b6bf09b                           failing        us-central1-b  10.0.4.44
mysql/28a210ac-cb98-4ab4-9672-9f4c661c57b8                           failing        us-central1-f  10.0.4.46
mysql/c1639373-26a2-44ce-85db-c9fe5a42964b                           failing        us-central1-c  10.0.4.45
proxy/87c5683d-12f5-426c-b925-62521529f64a                           running        us-central1-b  10.0.4.60
proxy/b0115ccd-7973-42d3-b6de-edb5ae53c63e                           running        us-central1-c  10.0.4.61
rejoin-unsafe/8ce9370a-e86b-4638-bf76-e103f858413f                   -              us-central1-b  -
smoke-tests/e026aaef-efd9-4644-8d14-0811cb1ba733                     -              us-central1-b  10.0.4.59
</pre>

In this situation, run the bootstrap errand:

1. Log in to the BOSH director.
1. Select the correct deployment.
1. Run one of the following commands depending on your Ops Manager version:<br><br>
    <ul>
        <li><strong>Ops Manager v1.11 or later:</strong> <code>bosh2 run-errand bootstrap</code> </li>
        <li><strong>Ops Manager v1.10:</strong> <code>bosh run errand bootstrap</code></li>
    </ul>


The above returns many lines of output, eventually followed by:

<pre class="terminal">
Bootstrap errand completed

[stderr]
+ echo 'Started bootstrap errand ...'
+ JOB_DIR=/var/vcap/jobs/bootstrap
+ CONFIG_PATH=/var/vcap/jobs/bootstrap/config/config.yml
+ /var/vcap/packages/bootstrap/bin/cf-mysql-bootstrap -configPath=/var/vcap/jobs/bootstrap/config/config.yml
+ echo 'Bootstrap errand completed'
+ exit 0

Errand `bootstrap' completed successfully (exit code 0)
</pre>
        
Sometimes this does not work immediately. If so, wait and try again a few minutes later.


## <a id="vms-terminated"></a>Scenario 2: Virtual Machines Terminated or Lost ##

In severe circumstances, such as a power failure, it is possible to lose all your VMs. You must recreate them before you can begin recovering the cluster. 

To determine the state of your VMs, run one of the following commands depending on your Ops Manager version:

<ul>
    <li><strong>Ops Manager v1.11 or later:</strong> <code>bosh2 -e YOUR-ENV instances</code> </li>
    <li><strong>Ops Manager v1.10:</strong> <code>bosh vms</code></li>
</ul>

The output resembles [this](#vms). If the VM is terminated or lost, the Process State for the `mysql` jobs is shown as `-`.

### <a id="vm-recovery"></a>Recover Terminated or Lost VMs

There are three steps to recover terminated or lost VMs:

1. [Recreate the Missing VMs](#recreate)
1. [Run the Bootstrap Errand](#run-the-errand)
1. [Restore the BOSH Configuration](#reset-deployment)

#### <a id="recreate"></a> Recreate the Missing VMs

In this procedure, you let BOSH create the
VMs, install the software on them, and attempt to start the jobs. These jobs will fail because the MySQL VMs fail when started if there is no active cluster for them to join. Therefore this procedure instructs BOSH to 
ignore the failing state of each VM to allow the software to be deployed to all VMs.

Choose one of the following procedures depending on your Ops Manager version.

**With Ops Manager v1.11 or Later**

1. Log in to the BOSH Director.
1. Download the current manifest by running `bosh2 -e MY-ENV -d MY-DEP > /tmp/manifest.yml`.
1. Run `bosh2 -e MY-ENV -d MY-DEP deploy /tmp/manifest.yml`. The deploy will fail to start the first MySQL VM. 
1. Run the command `bosh2 -e MY-ENV -d MY-DEP ignore mysql/INSTANCE_GUID`.
1. Repeat the Steps 3 and 4 until all instances have attempted to start.

**With Ops Manager v1.10**

1. Log in to the BOSH Director.
1. Target the correct deployment.
1. Run `bosh deploy` so that BOSH attempts to start one instance. The deploy will fail to start the first MySQL VM. 
1. Issue a `bosh ignore instance mysql/INSTANCE_GUID` command.
1. Repeat Steps 3 and 4 until all instances have attempted to start.

#### <a id="run-the-errand"></a>Run the Bootstrap Errand 

All instances now have a `failing` Process State, but also have the MySQL code installed on them.
In this state, the bootstrap process recovers the cluster.

1. Run one of the following commands depending on your Ops Manager version:<br><br>
    * **Ops Manager v1.11 or later:** `bosh2 run-errand bootstrap`
    * **Ops Manager v1.10:** `bosh run errand bootstrap`

1. Validate that the errand completes successfully.<br><br>
    - Some instances may still appear as `failing`. Ignore this and proceed to the next steps below.

#### <a id="reset-deployment"></a>Restore the BOSH Configuration 

<p class="note warning"><strong>Important:</strong> It is critical that you run the steps below. If you do not <i>unignore</i> all ignored 
instances, they are not updated in future deploys.</p>

To restore your BOSH configuration to its previous state, this procedure *unignores* each instance that was previously ignored:

1. For each ignored instance, run one of the following commands depending on your Ops Manager version:<br><br>
    * **Ops Manager v1.11 or later:** `bosh2 -e MY-ENV -d MY-DEP unignore mysql/INSTANCE_GUID`
    * **Ops Manager v1.10:** `bosh unignore instance mysql/INSTANCE_GUID`

1. Rerun one of these commands:<br><br>
    * **Ops Manager v1.11 or later:** `bosh2 -e MY-ENV deploy`
    * **Ops Manager v1.10:** `bosh deploy`

1. Validate that all `mysql` instances are in `running` state.


## <a id="manual-bootstrap"></a>Manual Bootstrapping 

1. If the bootstrap errand is not able to automatically recover the cluster, you may need to perform the steps manually. The following steps are prone to user-error and can result in lost data if followed incorrectly. Please follow the [assisted boostrap](#assisted-bootstrap) instructions above first, and only resort to the manual process if the errand fails to repair the cluster.

    1. SSH to each node in the cluster and, as root, shut down the `mariadb` process. To SSH into BOSH-deployed VMs, see the [Advanced Troubleshooting with the BOSH CLI](http://docs.pivotal.io/pivotalcf/1-11/customizing/trouble-advanced.html) topic.

            $ monit stop mariadb_ctrl

        Re-bootstrapping the cluster will not be successful unless all other nodes have been shut down.

    1. Choose a node to bootstrap.

        Find the node with the highest transaction sequence number (seqno). The sequence number of a stopped node can be retained by either reading the node's state file under `/var/vcap/store/mysql/grastate.dat`, or by running a mysqld command with a WSREP flag, like `mysqld --wsrep-recover`.<br><br>

        If a node shutdown gracefully, the seqno should be in the galera state file.

            $ cat /var/vcap/store/mysql/grastate.dat | grep 'seqno:'

        If the node crashed or was killed, the seqno in the galera state file should be `-1`. In this case, the seqno may be recoverable from the database. The following command will cause the database to start up, log the recovered sequence number, and then exit.

            $ /var/vcap/packages/mariadb/bin/mysqld --wsrep-recover

        **Note:** The galera state file will still say `seqno: -1` afterward.<br><br>

        Scan the error log for the recovered sequence number (the last number after the group id (uuid) is the recovered seqno):

            $ grep "Recovered position" /var/vcap/sys/log/mysql/mysql.err.log | tail -1
            150225 18:09:42 mysqld_safe WSREP: Recovered position e93955c7-b797-11e4-9faa-9a6f0b73eb46:15

        If the node never connected to the cluster before crashing, it may not even have a group id (uuid in grastate.dat). In this case there's nothing to recover. Unless all nodes crashed this way, don't choose this node for bootstrapping.


        #### <a id="set-bootstrap-node"></a>Bootstrap the first node ####

        Use the node with the highest `seqno` value as the new bootstrap node. If all nodes have the same `seqno`, you can choose any node as the new bootstrap node.


        <p class="note"><strong>Note:</strong> Only perform these bootstrap commands on the node with the highest seqno. Otherwise the node with the highest seqno will be unable to join the new cluster (unless its data is abandoned). Its mariadb process will exit with an error. See <a href="./architecture.html#behavior">cluster behavior</a> for more details on intentionally abandoning data.</p>

    1. On the new bootstrap node, update state file and restart the mariadb process:

            $ echo -n "NEEDS_BOOTSTRAP" > /var/vcap/store/mysql/state.txt
            $ monit start mariadb_ctrl

        You can check that the mariadb process has started successfully by running:

            $ watch monit summary

        It can take up to 10 minutes for monit to start the mariadb process.

        #### <a id="restart-nodes"></a>Restart the remaining nodes ####

    1. After the bootstrapped node is running, start the mariadb process on the remaining nodes via monit.

        Start the mariadb process:

            $ monit start mariadb_ctrl

        If the node is prevented from starting by the [Interruptor](interruptor.html), perform the manual procedure to force the node to rejoin the cluster, documented in [Pivotal Knowledge Base](https://discuss.pivotal.io/hc/en-us/articles/115014258668). 

        <p class="note warning"><strong>WARNING</strong>: Forcing a node to rejoin the cluster is a destructive procedure. Only perform it with the assistance of <a href="https://support.pivotal.io">Pivotal Support</a>.</p>

    1. Verify that the new nodes have successfully joined the cluster. The following command outputs the total number of nodes in the cluster:

            mysql> SHOW STATUS LIKE 'wsrep_cluster_size';
